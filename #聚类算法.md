# 聚类算法

## 监督学习&无监督学习




西瓜编号  |  色泽 | 根蒂 | 好瓜坏瓜（label）
:-----: |   :---: | :------: | :------:
1  | 青绿 | 稍卷 | 好瓜
2  |  乌黑 | 硬挺 | 坏瓜



### 监督学习
从给定的训练数据集中学习出一个函数（模型参数），当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集要求包括输入输出，也可以说是特征和目标。训练集中的目标是由人标注的。

#### 典型问题
* 分类
	若我们欲预测的是离散值，例如"好瓜" "坏瓜"，此类学习任务称为"分类" (classification)
* 回归
	若我们预测的是连续值，例如西瓜成熟度0.95，0.37，此类学习任务称为"回归" (regression)
	
#### 典型算法
* Logistic Regression
* BP神经网络算法
* 线性回归算法




### 无监督学习
在"无监督学习" (unsupervised learning) 中，训练样本的标记信息是未知的，目标是通过对无标记训练样本的学习来揭示数据的内在性质及规律，为进一步的数据分析提供基础.此类学习任务中研究最多、应用最广的是"聚类" (clustering)。


#### 典型问题
* 聚类
      <img src="C:\Users\Nick\Pictures\微信截图_20210421201407.png" style="zoom:70%;" />
	试图将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个"簇" (cluster). 通过这样的划分，每个簇可能对应于一些潜在的概念(类别) ，如"浅色瓜" "深色瓜"，"有籽瓜" "无籽瓜"，甚至"本地瓜""外地瓜"等;需说明的是，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名。
#### 典型算法
* K-Means算法
* DBSCAN算法




## 聚类

聚类试图将数据集中的样本划分为若干个互不相交的子集，每个子集称为一个"簇" (cluster). 通过这样的划分，每个簇可能对应于一些潜在的概念(类别) ，如"浅色瓜" "深色瓜"，"有籽瓜" "无籽瓜"，甚至"本地瓜""外地瓜"等;需说明的是，这些概念对聚类算法而言事先是未知的，聚类过程仅能自动形成簇结构，簇所对应的概念语义需由使用者来把握和命名。

聚类既能作为一个单独过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程.例如，在一些商业应用中需对新用户的类型进行判别， 且定义"用户类型"对商家来说却可能不太容易，此时往往可先对用户数据进行聚类，根据聚类结果将每个簇定义为一个类，然后再基于这些类训练分类模型，用于判别新用户的类型。



### 作用
  * 作为一种探索性分析方法，用来分析数据的内在特点，寻找数据的分布规律；
  * 作为分类的处理过程，并不直接进行数据分析，首先对需要分类的数据进行聚类，然后对聚类出的结果的每一个簇进行分类，实现数据的预处理。


### 性能度量
聚类性能度量亦称聚类"有效性指标" (validity index).。与监督学习中的性能度量作用相似，对聚类结果，我们需通过某种性能度量来评估其好坏;另一方面，若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好地得副符合要求的聚类结果。


我们希望的聚类结果：
* **簇内相似度高**
* **簇间相似度低**

#### 外部指标
将聚类结果与某个"参考模型" (reference model) 进行比较，称为"外部指标" (external dex)

<img src="C:\Users\Nick\Pictures\微信图片_20210421211138.png" style="zoom:70%;" />
Jaccard系数 ：
$$
JC = \frac{a}{a+b+c}
$$




#### 外部指标
直接考察聚类结果而不利用任何参考模型，称为"内部指标" (internal index)。

DBI指数：
$$
DBI = \frac{1}{k} \sum_{i=1}^k  max_{j \not= i} ( \frac{avg(C_i)+avg(C_j)}{dist(\mu_i,\mu_j)})
$$

DBI指数越小越好



### 距离计算
* 距离度量函数：dist（）：
	* 非负性:   $dist(x_i,x_j) \geq 0$
	* 同一性:   $dist(x_i,x_j) = 0$ 当且仅当 $x_i = x_j$
	* 对称'性:   $dist(x_i,x_j) = dist(x_j, x_i)$
	* 直递性:   $dist(x_i,x_j) \leq dist(x_i,x_k) + dist(x_k,x_j)$

* 闵可夫斯基距离:
给定样本$x_i = (x_i1;x_i2;...:x_in)$与$x_j = (x_j1;x_j2;...;x_jn)$
$$
dist_{mk}(x_i,x_j) = {(\sum_{u=1}^{n}{\mid x_{iu} - x_{ju} \mid}^p)}^{\frac{1}{p}}
$$

* 当p = 1时，闵可夫斯基距离即曼哈顿距离：
$$
dist_{man}(x_i,x_j) = {\mid \mid x_i - x_j \mid \mid }_1 = \sum_{u=1}^{n} \mid x_{iu} - x_{ju} \mid
$$
* 当p = 2时，闵可夫斯基距离即欧氏距离：
$$
dist_{ed}(x_i,x_j) = {\mid \mid x_i - x_j \mid \mid }_2 = \sqrt{\sum_{u=1}^{n} {\mid x_{iu} - x_{ju} \mid}^2}
$$

* 属性的划分
	* 连续属性
	* 离散属性

属性是否定义了“序”关系至关重要
例如：
	* 离散有序属性｛1，2，3｝
	* 离散无序属性｛飞机，火车，轮船｝


VDM距离
$$
VDM_p(a,b) = \sum_{i = 1}^{k} {\mid \frac{m_{u,a,i}}{m_{u,a}} - \frac{m_{u,b,i}}{m_{u,b}} \mid}^p
$$

于是，将闵可夫斯基距离和VDM结合即可处理混合属性。假定有$n_c$个有序属性，n-$n_c$个无序属性，，不是一般性，则有：
$$
MinkovDM_p(x_i,x_j) = {(\sum_{u=1}^{n_c}{\mid x_{iu} - x_{ju} \mid}^p + \sum_{u=n_c+1}^{n}VDM_p(x_{iu},x_{ju}))}^{\frac{1}{p}}
$$

## K-means算法
<img src="C:\Users\Nick\Pictures\微信图片_20210422165916.png" style="zoom:70%;" />